{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b29b8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor(), target_transform=None)\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaff2a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1875, 313)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5bef3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "print(train_features_batch.shape, train_labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e41918ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot one\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "random_index = torch.randint(0, len(train_features_batch), size=(1,)).item()\n",
    "image, label = train_features_batch[random_index], train_labels_batch[random_index]\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2cb5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "from torch import nn\n",
    "\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units:int, output_shape:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfb53002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "class_names = train_data.classes\n",
    "\n",
    "model_0 = FashionMNISTModelV0(\n",
    "    input_shape=28*28,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ")\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35cb674d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0315,  0.3171,  0.0531, -0.2525,  0.5959,  0.2112,  0.3233,  0.2694,\n",
       "         -0.1004,  0.0157]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_x = torch.rand([1, 1, 28, 28])\n",
    "model_0(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfd730fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading helper_functions.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d18a3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9be5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start:float, end: float, device:torch.device=None):\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device} is {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01ebf002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Trained on 0 / 60000 samples.\n",
      "Trained on 3200 / 60000 samples.\n",
      "Trained on 6400 / 60000 samples.\n",
      "Trained on 9600 / 60000 samples.\n",
      "Trained on 12800 / 60000 samples.\n",
      "Trained on 16000 / 60000 samples.\n",
      "Trained on 19200 / 60000 samples.\n",
      "Trained on 22400 / 60000 samples.\n",
      "Trained on 25600 / 60000 samples.\n",
      "Trained on 28800 / 60000 samples.\n",
      "Trained on 32000 / 60000 samples.\n",
      "Trained on 35200 / 60000 samples.\n",
      "Trained on 38400 / 60000 samples.\n",
      "Trained on 41600 / 60000 samples.\n",
      "Trained on 44800 / 60000 samples.\n",
      "Trained on 48000 / 60000 samples.\n",
      "Trained on 51200 / 60000 samples.\n",
      "Trained on 54400 / 60000 samples.\n",
      "Trained on 57600 / 60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:04<00:09,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.431, Test loss: 0.459, Test_acc: 83.836\n",
      "\n",
      "Epoch 1\n",
      "Trained on 0 / 60000 samples.\n",
      "Trained on 3200 / 60000 samples.\n",
      "Trained on 6400 / 60000 samples.\n",
      "Trained on 9600 / 60000 samples.\n",
      "Trained on 12800 / 60000 samples.\n",
      "Trained on 16000 / 60000 samples.\n",
      "Trained on 19200 / 60000 samples.\n",
      "Trained on 22400 / 60000 samples.\n",
      "Trained on 25600 / 60000 samples.\n",
      "Trained on 28800 / 60000 samples.\n",
      "Trained on 32000 / 60000 samples.\n",
      "Trained on 35200 / 60000 samples.\n",
      "Trained on 38400 / 60000 samples.\n",
      "Trained on 41600 / 60000 samples.\n",
      "Trained on 44800 / 60000 samples.\n",
      "Trained on 48000 / 60000 samples.\n",
      "Trained on 51200 / 60000 samples.\n",
      "Trained on 54400 / 60000 samples.\n",
      "Trained on 57600 / 60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:09<00:04,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.427, Test loss: 0.461, Test_acc: 84.075\n",
      "\n",
      "Epoch 2\n",
      "Trained on 0 / 60000 samples.\n",
      "Trained on 3200 / 60000 samples.\n",
      "Trained on 6400 / 60000 samples.\n",
      "Trained on 9600 / 60000 samples.\n",
      "Trained on 12800 / 60000 samples.\n",
      "Trained on 16000 / 60000 samples.\n",
      "Trained on 19200 / 60000 samples.\n",
      "Trained on 22400 / 60000 samples.\n",
      "Trained on 25600 / 60000 samples.\n",
      "Trained on 28800 / 60000 samples.\n",
      "Trained on 32000 / 60000 samples.\n",
      "Trained on 35200 / 60000 samples.\n",
      "Trained on 38400 / 60000 samples.\n",
      "Trained on 41600 / 60000 samples.\n",
      "Trained on 44800 / 60000 samples.\n",
      "Trained on 48000 / 60000 samples.\n",
      "Trained on 51200 / 60000 samples.\n",
      "Trained on 54400 / 60000 samples.\n",
      "Trained on 57600 / 60000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.423, Test loss: 0.463, Test_acc: 83.966\n",
      "\n",
      "Train time on cpu is 13.680 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install tqdm\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42)\n",
    "start_time = timer()\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Trained on {batch * len(X)} / {len(train_dataloader.dataset)} samples.\")\n",
    "\n",
    "    train_loss /= len(train_dataloader) # average loss per batch\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in test_dataloader:\n",
    "            test_pred = model_0(X_test)\n",
    "\n",
    "            test_loss += loss_fn(test_pred, y_test)\n",
    "            test_acc += accuracy_fn(y_test, test_pred.argmax(dim=1))\n",
    "\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    print(f\"Train loss: {train_loss:.3f}, Test loss: {test_loss:.3f}, Test_acc: {test_acc:.3f}\\n\")\n",
    "end_time = timer()\n",
    "\n",
    "total_train_time_model_0 = print_train_time(start_time, end_time, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be88ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
